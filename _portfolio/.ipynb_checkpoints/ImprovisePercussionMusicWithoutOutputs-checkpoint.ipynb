{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvise Percussion Music With an LSTM Network\n",
    " \n",
    "<img src=\"images/LSTM_cell.svg\" style=\"width:450;height:300px;\">\n",
    "\n",
    "We develop a model to create precussion music.  Since music is sequential, we use a specialized Recurrent Neural Network (RNN) called a [LSTM model](https://en.wikipedia.org/wiki/Long_short-term_memory) (Long short-term Memory) Model to learn the patterns of musical sequences.  We then use these learned patterns to generate new music. \n",
    "\n",
    "The type of music depends on a collection of music files in [MIDI format](https://en.wikipedia.org/wiki/MIDI).\n",
    "Each MIDI file corresponds to a musical piece, which is a series of notes over time.  In this example, we use the [Groove Dataset]() from TensorFlow.  However, in the appendix below, we show how to import files from a url.\n",
    "\n",
    "This project was based on the [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/nlp-sequence-models) by [deeplearning.ai](https://www.deeplearning.ai/).  However, we made the following significant changes:\n",
    "- our music files are percussion-based.  \n",
    "    - this changes the MIDI format to be based on Unpitched notes and PercussionChords\n",
    "    - these types have significantly less internet examples available than pitched notes\n",
    "    - this significantly changed the Data Exploration, Data Preparation, and some of the Generating Music sections.\n",
    "- we rely on standard libraries\n",
    "    - code from the course required customized libraries\n",
    "    - we use standard Python, Audio, and Tensorflow packages\n",
    "    - all customized functions were rewritten or coded in a different way\n",
    "- the data is cleaned before used to create the model\n",
    "    - we remove duplicates, short musical pieces, outliers, and unused features\n",
    "- added features\n",
    "    - display of the musical scores with MuseScore\n",
    "    - code to download midi files from a url in the appendix\n",
    "    - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Load Packages and Data](#load_packages)\n",
    "- [Data Exploration](#explore_data)\n",
    "- [Clean the Data](#clean_data) and remove outliers.\n",
    "- [Data Preparation](#data_preparation) includes extracting the data into time series, randomly selecting segments of the time series, and combining segments into 3D training example matrices.\n",
    "- [Build the Model](#build_model)\n",
    "- [Generate Music](#generate_music) using the trained model.\n",
    "- [Conclusion](#conclusion) and potential extensions.\n",
    "- [Appendix: Downloading MIDI Files](#maestro)\n",
    "- [References](#references) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='load'></a>\n",
    "## Load Packages and Data\n",
    "We load standard Python packages, TensorFlow packages, and Music packages.  Then, we load the MIDI files.\n",
    "\n",
    "### Load Standard Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy is the fundamental package for scientific computing with Python.\n",
    "import numpy as np\n",
    "\n",
    "# Pandas provides data structures and data analysis tools for Python\n",
    "import pandas as pd\n",
    "\n",
    "# Matplotlib is a Python 2D plotting library \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Seaborn is a Python data visualization library based on matplotlib.\n",
    "import seaborn as sns\n",
    "\n",
    "# Operating system dependent functionality\n",
    "\n",
    "# Implements binary protocols \n",
    "from io import BytesIO\n",
    "\n",
    "# Set the random seed\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "# Set directories\n",
    "location = '../files/'\n",
    "img_location = '../images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Audio Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fluidsynth\n",
    "import glob\n",
    "import pretty_midi\n",
    "import subprocess\n",
    "import tempfile\n",
    "from IPython.display import Image, Audio\n",
    "import music21\n",
    "\n",
    "# Specify the path to your FluidSynth soundfont (adjust this path accordingly)\n",
    "soundfont_path = '/usr/share/sounds/sf2/FluidR3_GM.sf2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Tensorflow Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TensorFlow is an open source machine learning framework.\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow Datasets is a collection of datasets ready to use with TensorFlow.\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# TensorFlow Model is a high-level API to build and train models in TensorFlow.\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# TensorFlow Layers builds neural network architectures.\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.layers import Dropout, Reshape, Lambda, RepeatVector, Flatten\n",
    "\n",
    "# TensorFlow Callbacks provides a set of functions that can be applied at different stages of training.\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set seed for reproducibility\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Set verbosity to low \n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Here, we use the [Groove dataset](https://www.tensorflow.org/datasets/catalog/groove) from the [TensorFlow Datasets](https://www.tensorflow.org/datasets) library. This dataset contains 13.6 hours of drumming performances from 10 drummers of various skill levels playing to a click track. The dataset contains 1,150 MIDI files and over 22,000 measures of drumming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full Groove dataset with MIDI only (no audio) as a tf.data.Dataset\n",
    "music_dataset = tfds.load(\n",
    "    name=\"groove/full-midionly\",\n",
    "    split=tfds.Split.TRAIN,\n",
    "    try_gcs=True)\n",
    "print(\"Type of Music Dataset\", type(music_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='explore_data'></a>\n",
    "## Explore the Data\n",
    "- [Extract First Few Music Files](#extract_samples)\n",
    "- [Explore Features of Music Files](#explore_features)\n",
    "    - [Numeric Features](#numeric_features)\n",
    "    - [Non-Numeric Features](#non_numeric_features)\n",
    "- [Explore Features of Percussion MIDI](#explore_midi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display Dataset\n",
    "\n",
    "Display the first few records of the dataset.  Each record of the Groove dataset corresponds to a song, and contains the following fields:\n",
    "  - **bpm**: beats per minute\n",
    "  - **drummer**: a unique number corresponding to the drummer who played the song\n",
    "  - **id**: a unique string iddentying the song\n",
    "  - **midi**: a sequence of musical notes\n",
    "  - **style/primary**: the style of the song\n",
    "  - **style/secondary**: the secondary style of the song\n",
    "  - **time_signature**: the time signature of the song\n",
    "  - **type**: the type of the song\n",
    "\n",
    "If the Groove dataset has been replaced by MIDI files with a different type (not purely percussion), \n",
    "then the fields will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas dataframe from the music dataset\n",
    "music_df = tfds.as_dataframe(music_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few records of the dataframe\n",
    "music_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='extract_samples'></a>\n",
    "### Extract First Few Music Files\n",
    "\n",
    "In the first step of exploring the data, we extract the first three music files from the dataset and save them as output_file0, output_file1, and output_file2 in wav format. We want to listen to the music files and get a sense of what the data looks like. We also want to see if there are any obvious differences between the music files. We will use the IPython library to display the audio files in the notebook and the MuseScore package to display the musical scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first few MIDI files from the music_df DataFrame\n",
    "#   NOT NEEDED if the music samples have already been extracted\n",
    "\n",
    "# Number of music files to explore in detail\n",
    "num_explore = 3\n",
    "\n",
    "# Loop through the first three MIDI files in music_df\n",
    "for i in range(num_explore):\n",
    "\n",
    "    # Extract the first MIDI file from music_df (assuming it's in bytes format)\n",
    "    midi_data = music_df.iloc[i]['midi']\n",
    "\n",
    "    # Create a BytesIO object to work with the MIDI data\n",
    "    midi_file = BytesIO(midi_data)\n",
    "\n",
    "    # Specify the local output MIDIfile path\n",
    "    output_mid_file = location + \"output_file\" + str(i) + \".mid\"\n",
    "    with open(output_mid_file, 'wb') as file:\n",
    "        file.write(midi_data)\n",
    "\n",
    "    # Create a temporary file to store the MIDI data\n",
    "    with tempfile.NamedTemporaryFile(suffix='.mid', delete=False) as temp_midi_file:\n",
    "        temp_midi_path = temp_midi_file.name\n",
    "        temp_midi_file.write(midi_data)\n",
    "\n",
    "    # Specify the local output WAV file path\n",
    "    output_wav_local_path = location + \"output_file\" + str(i) + \".wav\"\n",
    "\n",
    "    # Convert MIDI to WAV using FluidSynth and save to the local directory\n",
    "    subprocess.run(['fluidsynth', '-a', 'alsa', '-o', 'audio.alsa.device=default', '-F', output_wav_local_path, soundfont_path, temp_midi_path])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play Sample Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(location + \"output_file0.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(location + \"output_file1.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(location + \"output_file2.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Notation for Sample Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The second page of music notation for Audio 0:\")\n",
    "fig = Image(filename=(img_location + 'output_file0-2.png'))\n",
    "fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Music notation for the first page of output_file1\")\n",
    "fig = Image(filename=(img_location + 'output_file1-1.png'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Music notation for the first page of output_file2\")\n",
    "fig = Image(filename=(img_location +'output_file2-1.png'))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='explore_features'></a>\n",
    "### Explore the Features of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of columns in the dataset\n",
    "music_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of musical pieces in dataset:\", len(music_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='numeric_features'></a>\n",
    "#### Numeric Features\n",
    "Print statistics and plot histograms for numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics of the numeric features of the dataset\n",
    "music_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = music_df.select_dtypes(include=np.number)\n",
    "for col in df_num.columns:\n",
    "    sns.countplot(music_df, x=col).set_title(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlation of the Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Absolute correlation between numeric features.\")\n",
    "sns.heatmap(df_num.corr().abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highly correlated features should be removed\n",
    "print(\"Sorted Pairwise correlations: \")\n",
    "print(df_num.corr().abs().unstack().sort_values()[:-len(df_num.corr()):2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the features is highly correlated with the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='non_numeric_features'></a>\n",
    "#### Non-Numeric Features\n",
    "Print statistics and plot histograms for non-numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics for string columns\\n\")\n",
    "music_df.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that one of the musical pieces occurs twice in the datset.  We remove the duplicate in the 'Clean the Data' section.\n",
    "\n",
    "The id feature is a string that uniquely identifies each piece of music.  The distribution of the id feature is uniform and not interesting. \n",
    "The midi feature contains the music infomation, and a histogram of this data would not be helpful.  However, the style/secondary feature is a categorical variable that can be plotted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(music_df, x='style/secondary')\n",
    "plt.title('style/secondary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='explore_midi'></a>\n",
    "### Explore Midi Files\n",
    "\n",
    "Each MIDI file in this collection: \n",
    "- Starts with the same Header\n",
    "*b'MThd\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x01\\x01\\xe0'*\n",
    "- Contains a limited number of tracks, each starting with *b'MTrk'*\n",
    "- Each track contains a sequence of instruments, including *b'Midi Drums'*\n",
    "\n",
    "We add temporary features to explore the midi files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample MIDI data\n",
    "print(\"Sample Tracks (with header removed):\")\n",
    "for i in range(num_explore):\n",
    "    ex = music_df.loc[i, 'midi']\n",
    "    header, remainder = ex.split(b'MTrk')\n",
    "    print(\"\\t\", remainder)\n",
    "\n",
    "# Add temporary features to the datset\n",
    "print(\"Adding temporary features to the dataset\")\n",
    "music_df['Header'] = music_df['midi'].apply(lambda x: x.count(b'MThd\\x00\\x00\\x00\\x06\\x00\\x00\\x00\\x01\\x01\\xe0'))\n",
    "music_df['NumTracks'] = music_df['midi'].apply(lambda x: x.count(b'MTrk'))\n",
    "music_df['NumDrums'] = music_df['midi'].apply(lambda x: x.count(b'Midi Drums'))\n",
    "music_df['Brooklyn'] = music_df['midi'].apply(lambda x: x.count(b'Brooklyn'))\n",
    "\n",
    "# Display statistics of the temporary features\n",
    "tmp_features = ['Header', 'NumTracks', 'NumDrums', 'Brooklyn']\n",
    "music_df[tmp_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The temporary features confirm that each midi sequence starts with the same header and contains one track (starting with b'MTrk').  Each track contains 0, 1, or 2 instandce of b'Midi Drums' and 0 or 1 instantces of b'Brooklyn'.  After exploring the data, we no longer need these temporary features.  We can remove them from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df.drop(tmp_features, inplace=True, axis=1)   \n",
    "music_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the [music21 Library](https://web.mit.edu/music21/doc/moduleReference/moduleMidi.html#music21.midi.MidiFile) to process the MIDI sequences into \n",
    "[streams](https://web.mit.edu/music21/doc/moduleReference/moduleMidiTranslate.html#modulemiditranslate) and display a sample stream of music \n",
    "starting with the time of the note/chord and the type of note/chord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Music21 Stream of first MIDI sequence:\")\n",
    "\n",
    "s = music21.midi.translate.midiStringToStream(music_df.loc[0, 'midi'])\n",
    "s.show(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='clean_data'></a>\n",
    "## Clean the Data\n",
    "In this section, we \n",
    "- [Remove Duplicates](#remove_duplicates)\n",
    "- [Remove Outliers](#remove_outliers)\n",
    "- [Remove Features](#remove_features) that are not useful for our analysis\n",
    "- [Remove Short Pieces](#remove_short_pieces) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='remove_duplicates'></a>\n",
    "### Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='remove_outliers'></a>\n",
    "### Remove Outliers\n",
    "We remove the following outliers from the dataset:\n",
    "- Drummers. The drummers numbered 1, 3, 4, 5, 8, and 9 have a small number of samples. \n",
    "- Style/Primary. The style/primary numbered 0, 2, 4, 7, and 11 are not significant.\n",
    "- Time Signature. The time_signature feature is mostly 1.  The other values are insignificant.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = music_df.drop(music_df[music_df.drummer.isin([1, 3, 4, 5, 8, 9])].index)\n",
    "music_df = music_df.drop(music_df[music_df['style/primary'].isin([0, 2, 4, 7, 11])].index)\n",
    "music_df = music_df.drop(music_df[music_df.time_signature.isin([0, 2, 3, 4])].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='remove_features'></a>\n",
    "### Remove Unused Features\n",
    "\n",
    "The id feature uniquely identifies each song. It is not useful for our purposes, so we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df.drop(['id'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='remove_pieces'></a>\n",
    "### Remove Short Pieces\n",
    "We can't use short pieces of music in our training data.  We remove pieces of music with midi string length less than 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = music_df[music_df.midi.str.len().ge(500)]\n",
    "print(\"Remaining number of musical pieces in dataset:\", len(music_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='data_preparation'></a>\n",
    "## Data Preparation\n",
    "\n",
    "- [Parameters](#parameters)\n",
    "- [Load Midi Files](#load_midi)\n",
    "- [Create Time Series](#create_time_series)\n",
    "- [Represent Notes/Chords](#represent_notes)\n",
    "- [Create Model Input and Output](#model_input_output)\n",
    "\n",
    "<a name='parameters'></a>\n",
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_midi = 20 # number of midi files to use for training, \n",
    "Tx = 30   # number of time steps per input sequence\n",
    "Ty = 30   # number of time steps per output sequence\n",
    "mx = 400  # number of snippets of music to train on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='load_midi'></a>\n",
    "### Load Midi Files\n",
    "\n",
    "The Music21 library divides streams of music into:\n",
    "[scores](https://web.mit.edu/music21/doc/moduleReference/moduleStreamBase.html#music21.stream.base.Score), \n",
    "[parts](https://web.mit.edu/music21/doc/moduleReference/moduleStreamBase.html#music21.stream.base.Part), and \n",
    "[notes](https://web.mit.edu/music21/doc/moduleReference/moduleStreamBase.html#music21.stream.base.Stream.notes).\n",
    "\n",
    "Below, we create a list of musical scores.  Each score contains a list of parts.  Each part contains a list of notes/chords.  Each note contains a pitch and a duration, unless its an [Unpitched]() note.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originalScores = []\n",
    "quantize = True\n",
    "for i in range(num_midi):\n",
    "    j = music_df.index[i]\n",
    "    score = music21.converter.parseData(music_df.loc[j, 'midi'], quantizePost=quantize)\n",
    "    originalScores.append(score)\n",
    "\n",
    "print(\"Number of original scores: \", len(originalScores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='create_time_series'></a>\n",
    "### Create Time Series\n",
    "\n",
    "For each music piece, extract the [notes/chords](https://web.mit.edu/music21/doc/moduleReference/modulePercussion.html) and create a time series from these. In these percussion pieces, the notes/chords are of types \n",
    "- [PercussionChord](https://web.mit.edu/music21/doc/moduleReference/modulePercussion.html#music21.percussion.PercussionChord)\n",
    "    -- a series of notes played at the same time\n",
    "- [Unpitched](https://web.mit.edu/music21/doc/moduleReference/moduleNote.html#music21.note.Unpitched)\n",
    "\n",
    "Technical Note: If the [quantization option parse](http://web.mit.edu/music21/doc/moduleReference/moduleConverter.html#music21.converter.parse) is not turned off, the music21 library will round the time offset of the notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize time series \n",
    "time_series_dict = {}\n",
    "\n",
    "# Initialize chord dict\n",
    "chord_list = []\n",
    "chord_dict = {}\n",
    "chord_num = 0\n",
    "time_dict = {}\n",
    "instrument_set = set([])\n",
    "\n",
    "\n",
    "# For each musical piece in the originalScores list\n",
    "for i, piece in enumerate(originalScores[:num_midi]):\n",
    "\n",
    "    # Divide piece into parts\n",
    "    part_stream = piece.parts.stream()  \n",
    "   \n",
    "    # Initialize time step\n",
    "    time_note_pairs = []\n",
    "\n",
    "    # t0 is the previous time, which is \n",
    "    #    initialized to 0.0 for each piece\n",
    "    t0 = 0.0\n",
    "     \n",
    "    # Iterate through the notes in the Score\n",
    "    for n1 in piece.flatten().notes:\n",
    "  \n",
    "        # Check if the note is a PercussionChord\n",
    "        if isinstance(n1, music21.percussion.PercussionChord):\n",
    "\n",
    "            # Extract the names of the percussion instruments in the chord\n",
    "            instrument_list = [n2.getInstrument().instrumentName for n2 in n1.notes]\n",
    "            chord = '/'.join(instrument_list)\n",
    "            instrument_set.update(instrument_list)\n",
    "\n",
    "        # Else check if note is Unpitched    \n",
    "        elif isinstance(n1, music21.note.Unpitched):\n",
    "            # For Unpitched percussion notes\n",
    "            chord = n1.getInstrument().instrumentName\n",
    "            instrument_set.add(chord)\n",
    "          \n",
    "        # Else check if note is listed as a Voice stream   \n",
    "        elif isinstance(n1, music21.stream.Voice):\n",
    "            for item in n1:\n",
    "                if isinstance(item, music21.percussion.PercussionChord):\n",
    "                    instrument_list = [n2.getInstrument().instrumentName for n2 in item.notes]\n",
    "                    chord = '/'.join(instrument_list)\n",
    "                    instrument_set.update(instrument_list)\n",
    "                    \n",
    "                elif isinstance(item, music21.note.Unpitched):\n",
    "                    chord = item.getInstrument().instrumentName\n",
    "                    instrument_set.add(chord) \n",
    "\n",
    "        # If the chord is not yet in the dict, add it\n",
    "        if chord not in chord_dict.keys():\n",
    "            chord_dict[chord] = chord_num\n",
    "            time_dict[chord] = n1.offset - t0\n",
    "            chord_num += 1\n",
    "        t0 = n1.offset\n",
    "\n",
    "        # Add the chord number to the time series\n",
    "        time_note_pairs.append((n1.offset, chord_dict[chord]))     \n",
    "\n",
    "    # Record the time series for the piece\n",
    "    time_series_dict[i] = time_note_pairs\n",
    "    chord_list.append(pd.Series([s[0] for s in time_series_dict[i]]). unique().tolist())\n",
    "\n",
    "# Caclulate the number of unique chords\n",
    "nx = chord_num\n",
    "print(nx, \"Chords:\")\n",
    "\n",
    "# Print Time series info\n",
    "print(\"Number of time series\", len(time_series_dict))\n",
    "print(\"Sample time series\")\n",
    "for i in range(min(len(time_series_dict), 8)):\n",
    "    print(\"    Series\", i, \"length:\", len(time_series_dict[i]), time_series_dict[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='represent_notes'></a>\n",
    "### Represent notes/chords\n",
    "\n",
    "For each musical piece $p$ and each time $t$ in the time series, we represent the note/chord by a one-hot vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the vector representation of the note/chord at time step t in time series p\n",
    "def represent_note(p, k):\n",
    "\n",
    "    # Find the note/notes at time step k in the pth time series\n",
    "    chord_num = time_series_dict[p][k][1]\n",
    "   \n",
    "    # Initialize the note representation\n",
    "    representation = np.zeros(nx)\n",
    "\n",
    "    # Indicate which chord is being played\n",
    "    representation[chord_num] = 1\n",
    "\n",
    "    # Return the note representation\n",
    "    return representation\n",
    "\n",
    "\n",
    "# Test the represent_note function\n",
    "print(\"Testing: Representing the first 3 chords in the first time series:\")\n",
    "for j in range(3):\n",
    "    print(\"    \", represent_note(0, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='model_input_output'></a>\n",
    "### Create Model Input and Target\n",
    "\n",
    "The input and output to the model will have the following shapes:\n",
    "\n",
    "- `X` is an $(m_x, T_x, n_x)$ dimensional array. \n",
    "    - the first dimension indexes the **training example**\n",
    "    - the second dimension indexes **time** in the interval $[0, T_x)$\n",
    "    - the third dimension indexes **note values** in the set $\\{0, 1, \\ldots, n_x-1\\}$.\n",
    "        - each note value is represented as a one-hot vector. \n",
    "    - X[i,t,:] is a one-hot vector representing the value of the i-th example at time t. \n",
    "\n",
    "- `Y` is a $(T_y, m, n_x)$ dimensional array\n",
    "    - This is essentially the same as `X`, but shifted one step to the right (to the future)., and may contain additional notes.\n",
    "    - Notice that the data in `Y` is **reordered** to be dimension $(T_y, m, n_x)$. This format makes it more convenient to feed into the LSTM later.\n",
    "    - The sequence model will predict $Y = x^{\\langle t+1 \\rangle}, \\ldots, x^{\\langle t+T_y \\rangle}$ given $x^{\\langle 1\\rangle}, \\ldots, x^{\\langle t \\rangle}$. \n",
    "\n",
    "- We define the input X by randomly choosing $m$ time series of length $T_x$ from the time series of length $T_p$ for each piece of music $p$. \n",
    "- We define the target Y by shifting the input one time step to the right.  Thus, the target Y is the same as the input X, but shifted one time step to the right.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of consecutive time steps needed to extract input $X$ and target $Y$\n",
    "needed_time_steps = Ty + 1\n",
    "mx = 50  # number of snippets of music to train on\n",
    "\n",
    "# Make a list of all the potential times $t_0$ and pieces $p$ where the time series for $X$ can start\n",
    "potential_starts = []\n",
    "for i in range(num_midi):\n",
    "\n",
    "    piece_length = len(time_series_dict[i])\n",
    "    # If last time step is not long enough to extract X and Y, ignore the piece\n",
    "    if  piece_length >= needed_time_steps:\n",
    "        for j in range(piece_length - needed_time_steps):\n",
    "            potential_starts.append((i, j))\n",
    "\n",
    "# Randomly choose $mx$ of these potential starts\n",
    "potential_starts = np.array(potential_starts)\n",
    "np.random.shuffle(potential_starts)\n",
    "starts = potential_starts[:mx]\n",
    "\n",
    "# Create the training set from these randomly chosen start points\n",
    "X = [[] for _ in range(mx)]\n",
    "for k in range(mx):\n",
    "\n",
    "    i, j = starts[k]\n",
    "\n",
    "    # For each time step in the first $Tx$ time steps of the time series???\n",
    "    for t in range(Tx):\n",
    "\n",
    "        # Append the vector representation of the note/chord at time step $t$ in time series $p$ to the training set\n",
    "        X[k].append(represent_note(i, t + j))\n",
    "\n",
    "# Create the target set from these randomly chosen start points\n",
    "Y = [[] for _ in range(Ty)]\n",
    "for t in range(Ty):\n",
    "    for k in range(mx):\n",
    "        i, j = starts[k]\n",
    "        Y[t].append(represent_note(i, t + j + 1))\n",
    "\n",
    "# Convert the training set and target set to numpy arrays\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Print the shape of the training set and target set\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='building_model'></a>\n",
    "## Build the Model\n",
    "\n",
    "We  use an LSTM with hidden states that have $n_{a}$ dimensions.\n",
    "\n",
    "<img src=\"LSTM_Cell.svg\" style=\"width:600;height:400px;\">\n",
    "<caption><center><<b>Figure 1</b>: General LSTM model </center></caption>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of dimensions for the hidden state of each LSTM cell.\n",
    "n_a = 400\n",
    "\n",
    "# opt = Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, weight_decay=0.01)\n",
    "opt = Adam(learning_rate=0.0001)\n",
    "\n",
    "epoch_num = 200\n",
    "\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Sampling rate for audio playback\n",
    "_SAMPLING_RATE = 16000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DJ Model Function\n",
    "* we need to generate items in the sequence one at a time using $x^{\\langle t\\rangle} = y^{\\langle t-1 \\rangle}$. \n",
    "* The function `djmodel()` will call the LSTM layer $T_x$ times using a for-loop.\n",
    "* All $T_x$ copies have the same weights that aren't re-initialized.\n",
    "* the types of layers are:\n",
    "    * [Reshape()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape): Reshapes an output to a certain shape.\n",
    "    * [LSTM()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM): Long Short-Term Memory layer\n",
    "    * [Dense()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense): A regular fully-connected neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "reshaper = Reshape((1, nx))                  \n",
    "LSTM_cell = LSTM(n_a, return_state = True)        \n",
    "densor = Dense(nx, activation='softmax')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION: djmodel\n",
    "\n",
    "def djmodel(Tx, LSTM_cell, densor, reshaper):\n",
    "    \"\"\"\n",
    "    Implement the djmodel composed of Tx LSTM cells where each cell is responsible\n",
    "    for learning the following note based on the previous note and context.\n",
    "    Each cell has the following schema: \n",
    "            [X_{t}, a_{t-1}, c0_{t-1}] -> RESHAPE() -> LSTM() -> DENSE()\n",
    "    Arguments:\n",
    "        Tx -- length of the sequences in the corpus\n",
    "        LSTM_cell -- LSTM layer instance\n",
    "        densor -- Dense layer instance\n",
    "        reshaper -- Reshape layer instance\n",
    "    \n",
    "    Returns:\n",
    "        model -- a keras instance model with inputs [X, a0, c0]\n",
    "    \"\"\"\n",
    "    # Get the shape of input values\n",
    "    nx = densor.units\n",
    "    \n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input layer and specify the shape\n",
    "    X = Input(shape=(Tx, nx)) \n",
    "    \n",
    "    # Define the initial hidden state a0 and initial cell state c0 using `Input`\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    \n",
    "    # Define blocks\n",
    "    reshaper = Reshape((1, nx))                 \n",
    "    LSTM_cell = LSTM(n_a, return_state = True)         \n",
    "    densor = Dense(nx, activation='softmax')     \n",
    "    \n",
    "    # Create empty list to append the outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Loop over time steps in [0, Tx)]\n",
    "    for t in range(Tx):\n",
    "        \n",
    "        # Select the \"t\"th time step vector from X. \n",
    "        x = X[:,t,:]\n",
    "\n",
    "        # Reshape x to be (1, nx)\n",
    "        x = reshaper(x)\n",
    "        x = Dropout(0.05)(x)\n",
    "        \n",
    "        # Perform one step of the LSTM_cell\n",
    "        a, _, c = LSTM_cell(inputs=x, initial_state=[a, c])\n",
    "\n",
    "        # Apply densor to the hidden state output of LSTM_Cell\n",
    "        out = densor(a)\n",
    "\n",
    "        # Add the output to \"outputs\"\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # Create model instance\n",
    "    model = Model(inputs=[X, a0, c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = djmodel(Tx, LSTM_cell=LSTM_cell, densor=densor, reshaper=reshaper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Check the model layers\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model for training\n",
    "\n",
    "With options:\n",
    "    - optimizer: Adam optimizer\n",
    "    - Loss function: categorical cross-entropy (for multi-class classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a0 = np.zeros((mx, n_a))\n",
    "c0 = np.zeros((mx, n_a))\n",
    "history = model.fit([X, a0, c0], list(Y), validation_split=0.2, epochs=epoch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"loss at first epoch: {history.history['loss'][0]:.2}\")\n",
    "print(f\"loss at last epoch: {history.history['loss'][epoch_num -1]:.2}\")\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy measure is the first key that includes 'acc' \n",
    "for key in history.history.keys():\n",
    "    i = str(key).find('acc')\n",
    "    if i > -1:\n",
    "        print(key)\n",
    "        acc_measure = key\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"acc at first epoch: {history.history[acc_measure][0]:.2}\")\n",
    "print(f\"acc at last epoch: {history.history[acc_measure][epoch_num -1]:.2}\")\n",
    "# plt.plot(history.history['dense_17_accuracy'])\n",
    "plt.plot(history.history[acc_measure])\n",
    "plt.plot(history.history['val_' + str(acc_measure)])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validate'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='generate_music'></a>\n",
    "## Generate Music\n",
    "\n",
    "We use our trained model to synthesize new music. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='music_inference_model'></a>\n",
    "### Inference Model\n",
    "\n",
    "The function `music_inference_model()` samples a sequence of musical values by propagating the LSTM forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION: music_inference_model\n",
    "\n",
    "def music_inference_model(LSTM_cell, densor, Tp=100):\n",
    "    \"\"\"\n",
    "    Uses the trained \"LSTM_cell\" and \"densor\" from model() to generate a sequence of chords indicators.\n",
    "    \n",
    "    Arguments:\n",
    "    LSTM_cell -- the trained \"LSTM_cell\" from model(), Keras layer object\n",
    "    densor -- the trained \"densor\" from model(), Keras layer object\n",
    "    Tp -- integer, number of time steps to generate for the new piece\n",
    "    \n",
    "    Returns:\n",
    "    inference_model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the shape of input values\n",
    "    nx = densor.units\n",
    "\n",
    "    # Get the number of the hidden state vector\n",
    "    n_a = LSTM_cell.units\n",
    "    \n",
    "    # Define the input to the model with a shape \n",
    "    x0 = Input(shape=(1, nx))\n",
    " \n",
    "    # Define s0, initial hidden state for the decoder LSTM\n",
    "    a0 = Input(shape=(n_a,), name='a0')\n",
    "    c0 = Input(shape=(n_a,), name='c0')\n",
    "    a = a0\n",
    "    c = c0\n",
    "    x = x0\n",
    "\n",
    "    # Create an empty list of \"outputs\" \n",
    "    outputs = []\n",
    "    \n",
    "    # Loop over Tp and generate a value at every time step\n",
    "    for t in range(Tp):\n",
    "\n",
    "        # Perform one step of LSTM_cell\n",
    "        a, _, c = LSTM_cell(inputs=x, initial_state=[a,c])\n",
    "        \n",
    "        # Apply Dense layer to the hidden state output of the LSTM_cell \n",
    "        out = densor(a)\n",
    "\n",
    "        # Append the prediction to \"outputs\".\n",
    "        outputs.append(out)\n",
    " \n",
    "        # Select the next value according to \"out\",\n",
    "        # Set \"x\" to be the one-hot representation of the selected value\n",
    "        idx = tf.math.argmax(out, axis=-1)\n",
    "        x = tf.one_hot(idx, nx)\n",
    "      \n",
    "        # Use RepeatVector(1) to convert x into a tensor with shape=(None, 1, nx)\n",
    "        x = RepeatVector(1)(x)     \n",
    "        \n",
    "    # Create model instance \n",
    "    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)\n",
    "\n",
    "    return inference_model\n",
    "\n",
    "# Define an inference model with 50 values\n",
    "inference_model = music_inference_model(LSTM_cell, densor, Tp=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Check the inference model\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='predict_and_sample'></a>\n",
    "### Predict and Sample\n",
    "\n",
    "Use the inference model to predict an output `pred` which should be a list of length piece_length where each element is np (1, nx)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize x, a, and c as zeros\n",
    "x_0 = np.zeros((1, 1, nx))\n",
    "a_0 = np.zeros((1, n_a))\n",
    "c_0 = np.zeros((1, n_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# FUNCTION: predict_and_sample\n",
    "\n",
    "def predict_and_sample(inference_model, x_initializer = x_0, \n",
    "                       a_initializer = a_0, c_initializer = c_0):\n",
    "    \"\"\"\n",
    "    Predicts the next Tp values using the inference model.\n",
    "    \n",
    "    Arguments:\n",
    "    inference_model -- Keras model instance for inference time\n",
    "    x_initializer -- numpy array of shape (1, 1, nx), one-hot vector initializing the values generation\n",
    "    a_initializer -- numpy array of shape (1, n_a), initializing the hidden state of the LSTM_cell\n",
    "    c_initializer -- numpy array of shape (1, n_a), initializing the cell state of the LSTM_cel\n",
    "    \n",
    "    Returns:\n",
    "    results -- np array (Tp, nx), one-hot vectors representing the values generated\n",
    "    indices -- np array (Tp, 1), indices representing the values generated\n",
    "    \"\"\"\n",
    "    \n",
    "    nx = x_initializer.shape[2]\n",
    "    \n",
    "    # Use inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.\n",
    "    print(\"Shapes: x0, a0 \", x_initializer.shape, a_initializer.shape)\n",
    "    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])\n",
    "    \n",
    "    # Convert \"pred\" into an np.array() of indices with the maximum probabilities\n",
    "    print(\"   pred list \", len(pred))\n",
    "    indices = np.argmax(pred, axis=-1)\n",
    "\n",
    "    # Convert indices to one-hot vectors, the shape of the results should be (Tp, nx)\n",
    "    print(\"   indices \", indices.shape)\n",
    "    results = to_categorical(indices, num_classes=nx)\n",
    "    print(\"   results \", results.shape)\n",
    "   \n",
    "    return results, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='create_music'></a>\n",
    "### Create Music \n",
    "\n",
    "#### Map the Percussion Instruments to Pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "instrument_type_dict = {'Bass Drum': music21.instrument.BassDrum(),\n",
    "                        'Snare Drum': music21.instrument.SnareDrum(),\n",
    "                        'Tom Tom': music21.instrument.TomTom(),\n",
    "                        'Tom-Tom': music21.instrument.TomTom(),\n",
    "                        'Crash Cymbals': music21.instrument.CrashCymbals(),\n",
    "                        'High Hat Cymbal': music21.instrument.HiHatCymbal(),\n",
    "                        'Hi-Hat Cymbal': music21.instrument.HiHatCymbal(),\n",
    "                        'Vibraslap': music21.instrument.Vibraslap(),\n",
    "                        'Percussion': music21.instrument.Percussion()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_music(model, reversed_chord_dict, chord_dict, times, output_file):\n",
    "    \"\"\"\n",
    "    Generates music using a model trained to learn musical patterns\n",
    "    Creates an audio stream to save the music and play it.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- Keras model Instance, output of djmodel()\n",
    "    chord_list -- list of all the instruments used in the input set\n",
    "    output_file -- name of file to use for saving midi and wav files\n",
    "    \n",
    "    Returns:\n",
    "    predicted_tones -- python list containing predicted tones\n",
    "    \"\"\"\n",
    "    \n",
    "    # set up audio stream\n",
    "    out_stream = music21.stream.Stream()\n",
    "    out_stream.timeSignature = music21.meter.TimeSignature('4/4')\n",
    "    # Initialize tempo of the output stream with 130 bit per minute\n",
    "    out_stream.insert(0.0, music21.tempo.MetronomeMark(number=130))\n",
    "    # Add the key of C major\n",
    "    out_stream.insert(0.0, music21.key.Key('C'))\n",
    "    \n",
    "    # Initialize chord variables\n",
    "    curr_offset = 0.0     \n",
    "\n",
    "    # Choose the starting note and instrument\n",
    "    first_note = music21.note.Unpitched()\n",
    "    first_note.storedInstrument = instrument_type_dict['Bass Drum']\n",
    "    # first_note.displayInstrument = music21.instrument.BaseDrum()    \n",
    "    print(\"First note\", first_note.getInstrument().instrumentName)\n",
    "\n",
    "    # Insert the first note into our output stream\n",
    "    out_stream.insert(curr_offset, first_note)\n",
    "\n",
    "    # Generate a sequence of chords using the model\n",
    "    _, indices = predict_and_sample(inference_model)\n",
    "    indices = list(indices.squeeze())\n",
    "    # print(indices[:10])\n",
    "    chord_names = [reversed_chord_dict[p] for p in indices]\n",
    "    print(chord_names[:3])\n",
    "    \n",
    "    # Build the list of notes to play\n",
    "    for k in range(len(indices) - 1):\n",
    "        # Split the instruments in the chord\n",
    "        instrument_names = chord_names[k].split('/')\n",
    "\n",
    "        # Assign the time step associated with this chord\n",
    "        t = times[chord_names[k]]\n",
    "\n",
    "        # Insert the notes into the output stream\n",
    "        note_list = []\n",
    "        for inst in instrument_names:\n",
    "            unp = music21.note.Unpitched()\n",
    "            unp.storedInstrument = instrument_type_dict[inst]\n",
    "            unp.quarterLength = t + curr_offset\n",
    "            unp.quarterLength = t + curr_offset\n",
    "            note_list.append(unp)\n",
    "        # If only one note, add to output stream\n",
    "        if len(note_list) == 1:\n",
    "            out_stream.append(unp)\n",
    "        # If more than one note, add chord to output stream\n",
    "        else:\n",
    "            pChord = music21.percussion.PercussionChord(note_list)\n",
    "            out_stream.append(pChord)\n",
    "        # Update the time offset\n",
    "        curr_offset += t\n",
    "\n",
    "    # Save audio stream to file\n",
    "    mf = music21.midi.translate.streamToMidiFile(out_stream)\n",
    "    mf.open(output_file +\".midi\", 'wb')\n",
    "    mf.write()\n",
    "    print(\"Your generated music is saved in \" + output_file + \".midi\")\n",
    "    mf.close()\n",
    "    \n",
    "    return out_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "num_to_chord = dict([(x[1], x[0]) for x in chord_dict.items()])\n",
    "output_file = location + \"generated\"\n",
    "out_stream = generate_music(inference_model, num_to_chord, chord_dict, \n",
    "time_dict, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play Generated Audio\n",
    "\n",
    "Note that currently the audio is being played on a piano sound, regardless of the choice of instruments.  This is because the piano is the default instrument in the MIDI library.  We will need to change the instrument to the correct one before playing the music.  \n",
    "Each note represents a drum instrument.  \n",
    "Still working on this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the local output WAV file path\n",
    "output_wav_local_path = output_file + \".wav\"\n",
    "\n",
    "# Specifiy location of midi file\n",
    "temp_midi_path = output_file + \".midi\"\n",
    "\n",
    "# Convert MIDI to WAV using FluidSynth and save to the local directory\n",
    "subprocess.run(['fluidsynth', '-a', 'alsa', '-o', 'audio.alsa.device=default', '-F', output_wav_local_path, soundfont_path, temp_midi_path])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playthe audio of the generated piece\n",
    "Audio(location + 'generated.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play an audio of another generated piece\n",
    "Audio(location +'generatedsample2.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='conclusion'></a>\n",
    "## Conclusion and Extensions\n",
    "\n",
    "This model could be extended in many ways including the following:\n",
    "- Use a larger sample of pieces from the dataset\n",
    "- Use a more general form of MIDI files, not just Percussion\n",
    "- Use shorter pieces to train and to generate shorter pieces.\n",
    "- Use a random choice for the first note of the generated piece.\n",
    "- Handle time steps more robustly.\n",
    "- Use a more sophisticated model, such as a [Transformer](https://arxiv.org/abs/1706.03762) model.  \n",
    "- Optimize hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='maestro'></a>\n",
    "## Appendix: Downloading a Different Dataset\n",
    "\n",
    "In the demonstrtion above, we used data from the Groove dataset to create drumming pieces of music.  If you would rather create other types of music, you can download midi file collections from a website.  One such website is [MidiWorld](http://www.midiworld.com/files/).  You can download a collection of midi files from this website and use them to create music.  The code below shows how to download a collection of midi files from MidiWorld and use them to create music.\n",
    "\n",
    "Below, we demonstrate downloading files from the Maestro database, a collection of classical piano pieces.   This show how you could load other types of music into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Maestro Dataset\n",
    "[Code below](https://www.tensorflow.org/tutorials/audio/music_generation) is taken from the tensorflow tutorial on music generation. It downloads the Maestro Dataset, which could be used to generate classical piano pieces rather than using the Groove dataset to generate drumming pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN if you want to download the maestro dataset\n",
    "#   instead, you can use the Groove dataset, which is already extracted\n",
    "\n",
    "# Choose the version of the dataset you want to use\n",
    "maestro_file = 'maestro-v3.0.0-midi'\n",
    "version = 'v3.0.0'\n",
    "# Choose the location to download the dataset\n",
    "cur_dir = '/home/jenny/Downloads'\n",
    "data_dir = pathlib.Path(cur_dir + maestro_file)\n",
    "print(data_dir)\n",
    "\n",
    "# Download the dataset if it doesn't already exist\n",
    "if not data_dir.exists():  \n",
    "  tf.keras.utils.get_file(\n",
    "      maestro_file + '.zip]',\n",
    "      origin='https://storage.googleapis.com/magentadata/datasets/maestro/'+ version + '/' + maestro_file + '.zip',\n",
    "      extract=True,\n",
    "      cache_dir='.', cache_subdir='data',\n",
    "  )\n",
    "else:\n",
    "  print(\"Data already downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN if you want to extract the maestro dataset\n",
    "#   instead, you can use the Groove dataset, which is already extracted\n",
    "import patoolib\n",
    "complete_maestro_file = pathlib.Path(cur_dir + '/' + maestro_file)\n",
    "complete_maestro_zip = pathlib.Path(cur_dir + '/' + maestro_file + '.zip')\n",
    "print(complete_maestro_file)\n",
    "if not complete_maestro_file.exists():\n",
    "    patoolib.extract_archive(complete_maestro_zip, outdir=data_dir)\n",
    "    print(\"Maestro dataset extracted to \" + str(cur_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS FILE IF YOU WANT TO DOWNLOAD THE MAESTRO DATASET\n",
    "#  Otherwise, you can use the Groove dataset, which is already included in the repo\n",
    "print(complete_maestro_file)\n",
    "filenames = glob.glob(str(complete_maestro_file/'**/**/*.mid*'))\n",
    "print(filenames)\n",
    "print('Number of files:', len(filenames))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = filenames[0]\n",
    "print(sample_file)\n",
    "pm = pretty_midi.PrettyMIDI(sample_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SAMPLING_RATE = 16000\n",
    "\n",
    "def display_audio(pm: pretty_midi.PrettyMIDI, seconds=30):\n",
    "  waveform = pm.fluidsynth(fs=_SAMPLING_RATE)\n",
    "  # Take a sample of the generated waveform to mitigate kernel resets\n",
    "  waveform_short = waveform[:seconds*_SAMPLING_RATE]\n",
    "  return display.Audio(waveform_short, rate=_SAMPLING_RATE)\n",
    "\n",
    "display_audio(pm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='references'></a>\n",
    "## References\n",
    "\n",
    "- DeepLearning.AI Deep Learning Specialization.  This project is an expanded version of a class assignment from Course 5 of the specialization.\n",
    "- [TensorFlow Tutorial](https://www.tensorflow.org/tutorials/audio/music_generation)\n",
    "- [Maestro Dataset](https://magenta.tensorflow.org/datasets/maestro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W1-A3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
