{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTNn4zzAJWhA"
   },
   "source": [
    "# Word Embeddings and Bias\n",
    "\n",
    "In natural language processing (NLP), a [word embedding](https://en.wikipedia.org/wiki/Word_embedding) is a way of representing a word, usually with a real-valued vector.  Word embeddings that are close in the vector space are expected to correspond to words that are similar in meaning.\n",
    "\n",
    "Since word embeddings can be computationally expensive to calculate, many machine learning practitioners use pre-trained sets of word embeddings.  Here, we load a set of 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Load](#load) Packages and GloVe Word Embeddings]\n",
    "- Calculate difference between vectors with [Cosine Similarity](#cosine_similarity)\n",
    "- Use Linear Algebra to solve [Word Analogies](#word_analogies)\n",
    "- [Discover Bias](#discoverbias) in the word embeddings and represent the bias with a vector in the word embeddings vector space.\n",
    "- [Neutralize Bias](#neutralizebias) in words like \"receptionist\"\n",
    "- Apply [Equalization](#equalization) to pairs of words like \"actor\" and \"actress\" on either side of a bias (\"gender\" in this case) to make them equidistant from the bias vector.\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNlGgfN1JWhL"
   },
   "source": [
    "<a name='load'></a>\n",
    "## Load Packages and GloVe Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cWEywjgpJWhM",
    "outputId": "6b0b856a-d05d-4a6d-ba94-01d247254f20"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-CfRQ86JWhN"
   },
   "source": [
    "Below, we read in the words and word embeddings into two variables:\n",
    "- `words`: set of words in the vocabulary.\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with words and word embeddings\n",
    "glove_file = './GloVeWordEmbeddings.txt'\n",
    "with open(glove_file, 'r') as f:\n",
    "    words = set()\n",
    "    word_to_vec_map = {}\n",
    "\n",
    "    for line in f:\n",
    "        line = line.strip().split()\n",
    "        curr_word = line[0]\n",
    "        words.add(curr_word)\n",
    "        word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note About One-Hot Vectors as Word Embeddings\n",
    "In Machine Learning, categorical features are sometimes represented by one-hot vectors (also called indicator functions).  If each word was embedded with its one-hot representation, then the dimension of the vector space would be as large as the number of words.  This vector space would be large and unweildy.  Another problem is that the distance between any two one-hot vectors would be constant -- the distance would not represent the similarity in meaning.  Thus, one-hot representations do not make good word embeddings.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZBg2QwSJWhO"
   },
   "source": [
    "<a name='cosine_similarity'></a>\n",
    "## Cosine Similarity\n",
    "\n",
    "To measure the similarity between two words, we calculate the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) of their word embeddings (also called word vectors).\n",
    "Given two vectors $u$ and $v$, cosine similarity is defined as: \n",
    "\n",
    "$$\\text{CosineSimilarity(u, v)} = \\frac {u \\cdot v} {||u||_2 ||v||_2}$$\n",
    "\n",
    "where $u \\cdot v$ is the dot product (or inner product) of two vectors and \n",
    "$||u||_2 = \\sqrt{\\sum_{i=1}^{n} u_i^2}$ is the $L2$ norm of the vector $u$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mNjuTQ5JJWhP"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity is a measure of the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v, rounded to 4 decimal places\n",
    "    \"\"\"\n",
    "    \n",
    "    # Special case: u and v are the same\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "\n",
    "    # The dot product between u and v\n",
    "    dot_product = np.dot(u, v)\n",
    "    \n",
    "    # The L2 norm of u\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    \n",
    "    # The L2 norm of v\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    # Return 0 if the vectors are bo\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    return round(dot_product/ (norm_u * norm_v), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqH8_-3BJWhP",
    "outputId": "9454fe42-80ca-4b40-ea23-69ebac038fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(father, mother) =  0.8909\n",
      "cosine_similarity(grandfather, grandmother) =  0.8105\n",
      "cosine_similarity(ball, crocodile) =  0.2744\n",
      "cosine_similarity(france - paris, rome - italy) =  -0.6751\n",
      "cosine_similarity(france - paris, italy - rome) =  0.6751\n"
     ]
    }
   ],
   "source": [
    "# Get the embeddings for a few sample words\n",
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "grandfather = word_to_vec_map[\"grandfather\"]\n",
    "grandmother = word_to_vec_map[\"grandmother\"]\n",
    "\n",
    "# Calculate the cosine similarity between some of the words\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(grandfather, grandmother) = \", cosine_similarity(grandfather, grandmother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \", cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",\n",
    "      cosine_similarity(france - paris, rome - italy))\n",
    "print(\"cosine_similarity(france - paris, italy - rome) = \",\n",
    "      cosine_similarity(france - paris, italy - rome))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll test passed!\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_test(target):\n",
    "    a = np.random.uniform(-10, 10, 10)\n",
    "    b = np.random.uniform(-10, 10, 10)\n",
    "    c = np.random.uniform(-1, 1, 23)\n",
    "    \n",
    "    # Check cosine similarity of a vector with itself is 1\n",
    "    assert np.isclose(cosine_similarity(a, a), 1), \"cosine_similarity(a, a) must be 1\"\n",
    "    \n",
    "    # Check cosine similarity of the positive part of the vector with\n",
    "    #   the negative part of the vector is 0 -- they share no non-zero dimensions in common\n",
    "    assert np.isclose(cosine_similarity((c >= 0) * 1, (c < 0) * 1), 0), \"cosine_similarity(pos(a), neg(a)) must be 0\"\n",
    "    \n",
    "    # Check cosine similarity of a vector with its negative.  This should always be -1\n",
    "    assert np.isclose(cosine_similarity(a, -a), -1), \"cosine_similarity(a, -a) must be -1\"\n",
    "    \n",
    "    # Check that scaling both vectors by the same constant (2 in this case)\n",
    "    #   does not change the cosine similarity\n",
    "    assert np.isclose(cosine_similarity(a, b), cosine_similarity(a * 2, b * 4)), \"cosine_similarity must be scale-independent. You must divide by the product of the norms of each input\"\n",
    "\n",
    "    print(\"\\033[92mAll test passed!\")\n",
    "    \n",
    "cosine_similarity_test(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63Pjp_QSJWhQ"
   },
   "source": [
    "<a name='word_analogies'></a>\n",
    "## Word Analogies\n",
    "\n",
    "Word analogies are sentences of the form \n",
    "    <font color='brown'>\"*a* is to *b* as *c* is to *d*\"</font>\n",
    "where *a*, *b*, *c*, and *d* are words. An example is:  \n",
    "    <font color='brown'> '*man* is to *woman* as *boy* is to *girl*' </font>.\n",
    "    \n",
    "Word analogy completions have prompts of the form\n",
    "    <font color='brown'>\"*a* is to *b* as *c* is to **___**\"</font>\n",
    "where *a*, *b*, and *c* are words and you try to find the word that best fits the blank.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve word analogy completions by:\n",
    "* Using the word embeddings $e_a$, $e_b$, and $e_c$ of the words *a*, *b* and *c*\n",
    "* Finding the word embdding $e_d$ such that the cosine similarity between $e_b - e_a$ and $e_d - e_c$ is maximized\n",
    "* Return the word *d* associated with the embedding $e_d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity Function Adapted\n",
    "A version of the cosine similarity function in which the norm of the first vector is passed in.  \n",
    "This prevents that calculation from being done repetitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_adapted(u, norm_u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity is a measure of the degree of similarity between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)  \n",
    "        norm_u -- a float representing the L2 norm of u\n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v rounded to 4 decimal places\n",
    "    \"\"\"\n",
    "    \n",
    "    # Special case: u and v are the same\n",
    "    if np.all(u == v):\n",
    "        return 1\n",
    "\n",
    "    # The dot product between u and v\n",
    "    dot_product = np.dot(u, v)\n",
    "    \n",
    "    # The L2 norm of v\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    \n",
    "    # Avoid division by 0\n",
    "    if np.isclose(norm_u * norm_v, 0, atol=1e-32):\n",
    "        return 0\n",
    "    \n",
    "    return round(dot_product/ (norm_u * norm_v), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kGBV3yoQJWhS"
   },
   "outputs": [],
   "source": [
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy completion as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a, word_b, word_c -- words (strings)\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding embedding. \n",
    "    \n",
    "    Returns:\n",
    "    best word to fit in the blank of the word analogy completion\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert input words to lowercase\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings e_a, e_b and e_c \n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "   \n",
    "    # Initialize variables\n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "    vec_difference = e_b-e_a\n",
    "    vec_difference_norm = np.linalg.norm(vec_difference)\n",
    "    \n",
    "    # Loop over the word vector set\n",
    "    for w in words:   \n",
    "        # Avoid returning input word_c\n",
    "        if w == word_c:\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vec_difference (e_b - e_a) and the vector (e_w - e_c)\n",
    "        cosine_sim = cosine_similarity_adapted(vec_difference, vec_difference_norm, word_to_vec_map[w] - e_c)\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "        #    then set the new max_cosine_sim to the current cosine_sim and the best_word to the current word\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Complete Analogy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAll tests passed\n"
     ]
    }
   ],
   "source": [
    "def complete_analogy_test(target):\n",
    "    a = [3, 3] # Center at a\n",
    "    a_nw = [2, 4] # North-West oriented vector from a\n",
    "    a_s = [3, 2] # South oriented vector from a\n",
    "    \n",
    "    c = [-2, 1] # Center at c\n",
    "    # Create a controlled word to vec map\n",
    "    word_to_vec_map = {'a': a,\n",
    "                       'synonym_of_a': a,\n",
    "                       'a_nw': a_nw, \n",
    "                       'a_s': a_s, \n",
    "                       'c': c, \n",
    "                       'c_n': [-2, 2], # N\n",
    "                       'c_ne': [-1, 2], # NE\n",
    "                       'c_e': [-1, 1], # E\n",
    "                       'c_se': [-1, 0], # SE\n",
    "                       'c_s': [-2, 0], # S\n",
    "                       'c_sw': [-3, 0], # SW\n",
    "                       'c_w': [-3, 1], # W\n",
    "                       'c_nw': [-3, 2] # NW\n",
    "                      }\n",
    "    \n",
    "    # Convert lists to np.arrays\n",
    "    for key in word_to_vec_map.keys():\n",
    "        word_to_vec_map[key] = np.array(word_to_vec_map[key])\n",
    "            \n",
    "    assert(target('a', 'a_nw', 'c', word_to_vec_map) == 'c_nw')\n",
    "    assert(target('a', 'a_s', 'c', word_to_vec_map) == 'c_s')\n",
    "    assert(target('a', 'synonym_of_a', 'c', word_to_vec_map) != 'c'), \"Best word cannot be input query\"\n",
    "    assert(target('a', 'c', 'a', word_to_vec_map) == 'c')\n",
    "\n",
    "    print(\"\\033[92mAll tests passed\")\n",
    "    \n",
    "complete_analogy_test(complete_analogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnJJ_2sQJWhT"
   },
   "source": [
    "### Word Analogy Examples\n",
    "We run a few word analogy completions.  On some of these, the model does quite well on simple examples, but fails on more complex ones.  This is a common problem with word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some word analogies:\n",
      "\tsmall -> large :: tiny -> surpluses\n",
      "\tman -> woman :: boy -> girl\n",
      "\twet -> dry :: soak -> drain\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('small', 'large', 'tiny'),  ('man', 'woman', 'boy'), ('wet', 'dry', 'soak')]\n",
    "\n",
    "print(\"Word analogies:\")\n",
    "for triad in triads_to_try:\n",
    "    print ('\\t{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small -> tiny :: large -> tiny\n",
      "man -> male :: woman -> male\n",
      "sock -> foot :: shirt -> foot\n"
     ]
    }
   ],
   "source": [
    "triads_to_try = [('small', 'tiny', 'large'),  ('man', 'male', 'woman'), \n",
    "                 ('sock', 'foot', 'shirt'), ('whote', 'black', 'truth')]\n",
    "print(\"Example word analogies that aren't successful:\")\n",
    "for triad in triads_to_try:\n",
    "    print ('\\t{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad, word_to_vec_map)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMRD25MuJWhW"
   },
   "source": [
    "<a name='discoverbias'></a>\n",
    "# Discover Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Q1w3ZpEJWhW"
   },
   "source": [
    "Bias, including gender bias, can be reflected in a word embedding.  We begin by computing a vector $g = e_{woman}-e_{man}$, where $e_{woman}$ represents the word vector corresponding to the word *woman*, and $e_{man}$ corresponds to the word vector corresponding to the word *man*. The resulting vector $g$ roughly encodes the concept of \"gender\".   We might get a more accurate representation if you compute $g_1 = e_{mother}-e_{father}$, $g_2 = e_{girl}-e_{boy}$, etc. and average over them, but just using $e_{woman}-e_{man}$ will illustrate the concept well enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "_qpU-C3KJWhW"
   },
   "outputs": [],
   "source": [
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORIvi0s1JWhX"
   },
   "source": [
    "Now, consider the cosine similarity of different words with $g$. What does a positive value of similarity mean, versus a negative cosine similarity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TgqV6pDxJWhX",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of gender differences and their similarities with constructed vector:\n",
      "queen - king similarity with g: 0.597\n",
      "girl - boy similarity with g: 0.6695\n",
      "woman - man similarity with g: 1\n"
     ]
    }
   ],
   "source": [
    "print('List of gender differences and their similarities with constructed vector:')\n",
    "\n",
    "# girls and boys name\n",
    "pair_list = [('queen', 'king'), ('girl', 'boy'), ('woman', 'man')]\n",
    "for pair in pair_list:\n",
    "    e_diff = word_to_vec_map[pair[0]] - word_to_vec_map[pair[1]]\n",
    "    print(\"\\t\", pair[0], '-', pair[1], 'similarity with g:', cosine_similarity(e_diff, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with gender vector:\n",
      "\t john -0.2316\n",
      "\t marie 0.3156\n",
      "\t sophie 0.3187\n",
      "\t rishi -0.0926\n",
      "\t priya 0.1763\n",
      "\t rahul -0.1692\n",
      "\t danielle 0.2439\n",
      "\t reza -0.0793\n",
      "\t katy 0.2831\n",
      "\t bree -0.0006\n"
     ]
    }
   ],
   "source": [
    "print ('Some names and their similarities with gender vector:')\n",
    "\n",
    "# girls and boys name\n",
    "name_list = ['john', 'marie', 'sophie', 'rishi', 'priya', 'rahul', 'danielle', 'reza', 'katy', 'bree']\n",
    "for w in name_list:\n",
    "    print (\"\\t\", w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLELp0LJJWhY"
   },
   "source": [
    "As you can see, female first names tend to have a positive cosine similarity with our constructed vector $g$, while male first names tend to have a negative cosine similarity. This is not surprising, and the result seems acceptable. \n",
    "\n",
    "Now try with some other words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "wgadfCaGJWhY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other words and their similarities:\n",
      "mathematics -0.011\n",
      "data -0.0153\n",
      "science -0.0608\n",
      "arts 0.0082\n",
      "literature 0.0647\n",
      "drawing -0.1109\n",
      "doctor 0.119\n",
      "tree -0.0709\n",
      "receptionist 0.3308\n",
      "technology -0.1319\n",
      "fashion 0.0356\n",
      "teacher 0.1792\n",
      "engineer -0.0804\n",
      "artist 0.0968\n",
      "computer -0.1033\n",
      "singer 0.185\n"
     ]
    }
   ],
   "source": [
    "print('Other words and their similarities:')\n",
    "word_list = ['mathematics', 'data', 'science', 'arts', 'literature', 'drawing','doctor', 'tree', 'receptionist', \n",
    "             'technology',  'fashion', 'teacher', 'engineer', 'artist', 'computer', 'singer']\n",
    "for w in word_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUzfNtq4JWhY"
   },
   "source": [
    "These results reflect gender stereotypes. For example, we see “computer” is negative and is closer in value to male first names, while “literature” is positive and is closer to female first names. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='neutralizebias'></a>\n",
    "# Neutralize Bias\n",
    "\n",
    "You'll see below how to . Note that some word pairs such as \"actor\"/\"actress\" or \"grandmother\"/\"grandfather\" should remain gender-specific, while other words such as \"receptionist\" or \"technology\" should be neutralized, i.e. not be gender-related. You'll have to treat these two types of words differently when debiasing.\n",
    "\n",
    "Each word embedding can be split into two parts: a multiple of the bias-direction $g$, and an orthogonal vector in the remaining dimensions $g_{\\perp}$ here.\n",
    "We neutralize a vector such as $e_{receptionist}$ by zeroing out the component in the direction of $g$, giving us $e_{receptionist}^{debiased}$. This algorithm is due to [Boliukbasi et al., 2016](https://arxiv.org/abs/1607.06520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "79Pk0QDhJWhZ"
   },
   "outputs": [],
   "source": [
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias vector. \n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. \n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula given above. \n",
    "    e_biascomponent = g * np.dot(g, e)/np.dot(g,g)\n",
    " \n",
    "    # Neutralize e by subtracting e_biascomponent from it \n",
    "    e_debiased = e - e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6PgkxwxXJWhZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.3308\n",
      "cosine similarity between receptionist and g, after neutralizing:  -0.0\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Da--RZGIJWha"
   },
   "source": [
    "The second result is essentially 0, up to numerical rounding (on the order of $10^{-17}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciATgxwLJWha"
   },
   "source": [
    "<a name='equalization'></a>\n",
    "# Equalization Algorithm\n",
    "\n",
    "We apply equalization to word pairs such as \"actress\" and \"actor\" to convert them to vectors that differ only through the gender property. The key idea behind equalization is to make sure that a particular pair of words are equidistant from $g_\\perp$. The equalization step also ensures that the two equalized steps are now the same distance from $e_{receptionist}^{debiased}$, or from any other work that has been neutralized. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first step of equalization is to average the two embeddings.\n",
    "$$\\mu = \\frac {{e_{w1}} + {e_{w2}}}{2}$$ \n",
    "\n",
    "Then, we project the average vector $\\mu$ and both the original embeddints onto the bias vector $g$.\n",
    "This produces the bias component of the average vector $\\mu_B$ and the bias components of the two original embeddings $e_{w1B}$ and $e_{w2B}$\n",
    "$$\\mu_{B} = \\frac {\\mu\\cdot g} {||g||_2^2}*g$$ \n",
    "$$ e_{w1B} = \\frac {e_{w1} \\cdot g}{||g||_2^2} *g$$ \n",
    "$$ e_{w2B} = \\frac {e_{w2} \\cdot g}{||g||_2^2} *g$$\n",
    "\n",
    "We find the component of $\\mu$ that is orthogonal to the bias vector $g$.\n",
    "$$\\mu_{\\perp} = \\mu - \\mu_{B}$$\n",
    "\n",
    "Next, we correct the two embeddings to \n",
    "$$e_{w1B}^{corrected} =  \\frac{\\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |}} {||(e_{w1} - \\mu_{\\perp}) - \\mu_B||_2}(e_{\\text{w1B}} - \\mu_B)$$\n",
    "$$e_{w2B}^{corrected} =  \\frac{\\sqrt{ |{1 - ||\\mu_{\\perp} ||^2_2} |}} {||(e_{w2} - \\mu_{\\perp}) - \\mu_B||_2}(e_{\\text{w2B}} - \\mu_B)$$\n",
    "\n",
    "Finally, we calculate the two equalized embeddings $e_1$ and $e_2$.\n",
    "$$e_1 = e_{w1B}^{corrected} + \\mu_{\\perp}$$\n",
    "$$e_2 = e_{w2B}^{corrected} + \\mu_{\\perp}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aBhxJtGIJWha"
   },
   "outputs": [],
   "source": [
    "def equalize(pair, bias_axis, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Debias pairs of words by following the equalize method described in the figure above.\n",
    "    \n",
    "    Arguments:\n",
    "    pair -- pair of strings to debias, e.g. (\"actress\", \"actor\") \n",
    "    bias_axis -- numpy-array, vector corresponding to the bias axis, e.g. gender\n",
    "    word_to_vec_map -- dictionary mapping words to their corresponding vectors\n",
    "    \n",
    "    Returns\n",
    "    e_1 -- word vector corresponding to the first word\n",
    "    e_2 -- word vector corresponding to the second word\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\"\n",
    "    w1, w2 = pair\n",
    "    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]\n",
    "    \n",
    "    # Compute the mean of e_w1 and e_w2\n",
    "    mu = (e_w1 + e_w2)/2\n",
    "\n",
    "    # Compute the projections of mu over the bias axis and the orthogonal axis\n",
    "    mu_B = (np.dot(mu, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "    mu_orth = mu - mu_B\n",
    "\n",
    "    # Compute e_w1B and e_w2B \n",
    "    e_w1B = (np.dot(e_w1, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "    e_w2B = (np.dot(e_w2, bias_axis) / np.linalg.norm(bias_axis)**2) * bias_axis\n",
    "        \n",
    "    # Step 5: Adjust the Bias part of e_w1B and e_w2B \n",
    "    c = np.sqrt(np.abs(1-np.linalg.norm(mu_orth)**2))\n",
    "    corrected_e_w1B = c*((e_w1B - mu_B)/np.linalg.norm((e_w1-mu_orth)-mu_B))\n",
    "    corrected_e_w2B = c*((e_w2B - mu_B)/np.linalg.norm((e_w2-mu_orth)-mu_B))\n",
    "\n",
    "    # Step 6: Debias by equalizing e1 and e2 to the sum of their corrected projections (≈2 lines)\n",
    "    e1 = corrected_e_w1B + mu_orth\n",
    "    e2 = corrected_e_w2B + mu_orth\n",
    "     \n",
    "    return e1, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "P405J5ZSJWhb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"man\"], gender) =  -0.1171\n",
      "cosine_similarity(word_to_vec_map[\"woman\"], gender) =  0.3567\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.7004\n",
      "cosine_similarity(e2, gender) =  0.7004\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"man\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"man\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"woman\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"woman\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"man\", \"woman\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarities before equalizing:\n",
      "cosine_similarity(word_to_vec_map[\"actor\"], gender) =  -0.0839\n",
      "cosine_similarity(word_to_vec_map[\"actress\"], gender) =  0.3342\n",
      "\n",
      "cosine similarities after equalizing:\n",
      "cosine_similarity(e1, gender) =  -0.6083\n",
      "cosine_similarity(e2, gender) =  0.6083\n"
     ]
    }
   ],
   "source": [
    "print(\"cosine similarities before equalizing:\")\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"actor\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"actor\"], g))\n",
    "print(\"cosine_similarity(word_to_vec_map[\\\"actress\\\"], gender) = \", cosine_similarity(word_to_vec_map[\"actress\"], g))\n",
    "print()\n",
    "e1, e2 = equalize((\"actor\", \"actress\"), g, word_to_vec_map)\n",
    "print(\"cosine similarities after equalizing:\")\n",
    "print(\"cosine_similarity(e1, gender) = \", cosine_similarity(e1, g))\n",
    "print(\"cosine_similarity(e2, gender) = \", cosine_similarity(e2, g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXi96ToSJWhc"
   },
   "source": [
    "These debiasing algorithms are very helpful for reducing bias, but aren't perfect and don't eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction $g$ was defined using only the pair of words _woman_ and _man_. As discussed earlier, if $g$ were defined by computing $g_1 = e_{woman} - e_{man}$; $g_2 = e_{mother} - e_{father}$; $g_3 = e_{girl} - e_{boy}$; and so on and averaging over them, you would obtain a better estimate of the \"gender\" dimension in the 50 dimensional word embedding space. Feel free to play with these types of variants as well! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY2ZhorNJWhd"
   },
   "source": [
    "<a name='6'></a>\n",
    "## 6 - References\n",
    "\n",
    "- The debiasing algorithm is from Bolukbasi et al., 2016, [Man is to Computer Programmer as Woman is to\n",
    "Homemaker? Debiasing Word Embeddings](https://papers.nips.cc/paper/6228-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings.pdf)\n",
    "- The GloVe word embeddings were due to Jeffrey Pennington, Richard Socher, and Christopher D. Manning. (https://nlp.stanford.edu/projects/glove/)\n",
    "- This material adapted from an assignment in DeepLearning.AI's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "DLSC5W2-A1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
